{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Dpw-sd0IYJA"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFy3zax0moUZ"
      },
      "outputs": [],
      "source": [
        "!pip install nflows --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrtadgRDIpJk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pylab import savefig\n",
        "from scipy.io import arff\n",
        "import ntpath\n",
        "import glob\n",
        "import os\n",
        "import math\n",
        "from sklearn import preprocessing\n",
        "# !pip install liac-arff\n",
        "#import arff\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from nflows.flows import Flow\n",
        "from nflows.distributions import StandardNormal\n",
        "from nflows.transforms import CompositeTransform, MaskedAffineAutoregressiveTransform, RandomPermutation\n",
        "from nflows.transforms import AffineCouplingTransform\n",
        "\n",
        "from sklearn import manifold\n",
        "import string\n",
        "\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPH75F5p9O31"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load object from file\n",
        "with open(gdrivePath +os.sep + \"data\" +os.sep+'X_train.pkl', 'rb') as f:\n",
        "    X_train = pickle.load(f)\n",
        "with open(gdrivePath +os.sep + \"data\" +os.sep+'X_test.pkl', 'rb') as f:\n",
        "    X_test = pickle.load(f)\n",
        "with open(gdrivePath +os.sep + \"data\" +os.sep+'y_train.pkl', 'rb') as f:\n",
        "    y_train = pickle.load(f)\n",
        "with open(gdrivePath +os.sep + \"data\" +os.sep+'y_test.pkl', 'rb') as f:\n",
        "    y_test = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVuN8FDW1sKJ"
      },
      "outputs": [],
      "source": [
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGW_I_CzMuQL"
      },
      "outputs": [],
      "source": [
        "unique_values, counts = np.unique(y_train, return_counts=True)\n",
        "display(dict(zip(unique_values, counts)),np.max(counts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yqGGdzBq75k"
      },
      "outputs": [],
      "source": [
        "def FB_Oversampler(num_synthetic_samples, X_min,num_layers,hidden_features,learning_rate):\n",
        "\n",
        "\n",
        "  data = torch.tensor(X_min, dtype=torch.float32)\n",
        "\n",
        "  # Define the number of features and hidden features\n",
        "  num_features = X_min.shape[1]\n",
        "  #hidden_features = 128  # number of hidden units in the neural network\n",
        "\n",
        "  # a sequence of masked affine autoregressive transforms\n",
        "  #num_layers = 2\n",
        "\n",
        "  # Base distribution: standard normal\n",
        "  base_distribution = StandardNormal([num_features])\n",
        "\n",
        "  # the sequence of transformations\n",
        "  transforms = []\n",
        "  for _ in range(num_layers):\n",
        "      transforms.append(MaskedAffineAutoregressiveTransform(features=num_features, hidden_features=hidden_features)) #MaskedAffineAutoregressiveTransform(features=num_features, hidden_features=hidden_features)\n",
        "      transforms.append(RandomPermutation(features=num_features))\n",
        "  transform = CompositeTransform(transforms)\n",
        "\n",
        "  #  the flow model\n",
        "  flow = Flow(transform, base_distribution)\n",
        "\n",
        "  # Train the model\n",
        "  optimizer = optim.Adam(flow.parameters(), lr=learning_rate)\n",
        "  num_epochs = 50\n",
        "  batch_size = 128\n",
        "\n",
        "\n",
        "  data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "      for batch in data_loader:\n",
        "          optimizer.zero_grad()\n",
        "          loss = -flow.log_prob(batch).mean()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "      #print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
        "\n",
        "  # Generate new samples\n",
        "  with torch.no_grad():\n",
        "      samples = flow.sample(num_synthetic_samples)\n",
        "  generated_data_np = samples.numpy()\n",
        "\n",
        "  return generated_data_np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84I6iweNIfYB"
      },
      "outputs": [],
      "source": [
        "# Get unique values and their counts\n",
        "unique_values, counts = np.unique(y_train, return_counts=True)\n",
        "classlabel_counts = dict(zip(unique_values, counts))\n",
        "max_count = 759\n",
        "\n",
        "i=1\n",
        "for label, count in classlabel_counts.items():\n",
        "    #print(x.shape, y.shape)\n",
        "    #print(\"label, count, max_count\",label,count,max_count)\n",
        "    X_minority = X_train[y_train == label]\n",
        "    #X_ori_min = X_train[y_train == label]\n",
        "    #print(\"Real\",X_minority.min(), X_minority.max())\n",
        "    if count < max_count:\n",
        "        #print(\"\\n\")\n",
        "        print(f\"Value {label} appears {count} times.\")\n",
        "        num_synthetic_samples = max_count - count\n",
        "        synthetic_samples = FB_Oversampler(int(num_synthetic_samples), X_minority, 2, 64, 1e-6) #num_layers,hidden_features,learning_rate\n",
        "        if i==1:\n",
        "          X_train_gen = X_minority\n",
        "          X_train_gen = np.vstack([X_train_gen, synthetic_samples])\n",
        "\n",
        "          y_train_gen = np.full(759, label)\n",
        "\n",
        "          y_train_indexes = np.full(count, 1)\n",
        "          y_train_indexes = np.concatenate([y_train_indexes, np.full(num_synthetic_samples, 2)])\n",
        "        else:\n",
        "          tmp = np.vstack([X_minority, synthetic_samples])\n",
        "          X_train_gen = np.vstack([X_train_gen, tmp])\n",
        "          y_train_gen = np.concatenate([y_train_gen, np.full(759, label)])\n",
        "\n",
        "          y_train_indexes = np.concatenate([y_train_indexes, np.full(count, 1)])\n",
        "          y_train_indexes = np.concatenate([y_train_indexes, np.full(num_synthetic_samples, 2)])\n",
        "    else:\n",
        "        X_train_gen = np.vstack([X_train_gen, X_minority])\n",
        "        y_train_gen = np.concatenate([y_train_gen, np.full(count, label)])\n",
        "        y_train_indexes = np.concatenate([y_train_indexes, np.full(count, 1)])\n",
        "    i=i+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WKo4iP-9eEZ"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Save object to file\n",
        "with open(gdrivePath +os.sep+\"results\"+os.sep+\"FB\"+os.sep+'FB_X_train.pkl', 'wb') as f:\n",
        "    pickle.dump(X_train_gen, f)\n",
        "\n",
        "with open(gdrivePath +os.sep+\"results\"+os.sep+\"FB\"+os.sep+'FB_X_test.pkl', 'wb') as f:\n",
        "    pickle.dump(X_test, f)\n",
        "\n",
        "with open(gdrivePath +os.sep+\"results\"+os.sep+\"FB\"+os.sep+'FB_y_train.pkl', 'wb') as f:\n",
        "    pickle.dump(y_train_gen, f)\n",
        "\n",
        "with open(gdrivePath +os.sep+\"results\"+os.sep+\"FB\"+os.sep+'FB_y_test.pkl', 'wb') as f:\n",
        "    pickle.dump(y_test, f)\n",
        "\n",
        "with open(gdrivePath +os.sep+\"results\"+os.sep+\"FB\"+os.sep+'FB_y_train_idx.pkl', 'wb') as f:\n",
        "    pickle.dump(y_train_indexes, f)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}