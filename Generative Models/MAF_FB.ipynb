{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "MAF-FB Runner (Flow-based, Masked Affine Autoregressive Transform – Baseline)\n",
        "\n",
        "This script trains a MAF-FB flow-based model for single-cell data.\n",
        "\n",
        "Default hyperparameters\n",
        "\n",
        "epochs: 100\n",
        "\n",
        "batch size: 128\n",
        "\n",
        "hidden features: 1024\n",
        "\n",
        "learning rate: 1e-6\n",
        "\n",
        "PBMC3K\n",
        "\n",
        "Trains MAF-FB on real train samples (100 epc, bs=128, hidden=1024, lr=1e-6).\n",
        "\n",
        "Generates synthetic samples equal to the TEST sample size.\n",
        "\n",
        "Saves: pbmc3k_MAF-FB.pkl\n",
        "\n",
        "PBMC68K\n",
        "\n",
        "Uses the processed PBMC68K data downloaded from the ACTIVA repository (unchanged).\n",
        "\n",
        "Trains MAF-FB on train split with the same hyperparameters.\n",
        "\n",
        "Generates synthetic samples equal to the TEST sample size.\n",
        "\n",
        "Saves: pbmc68k_MAF-FB.pkl\n",
        "\n",
        "HCA-BM10K (5-fold CV)\n",
        "\n",
        "Integrated Pancreatic Dataset (5-fold CV)\n",
        "\n",
        "For EACH FOLD:\n",
        "\n",
        "Fit MAF-FB on TRAIN ONLY.\n",
        "\n",
        "Generate synthetic samples per class to reach the Q3 (75th percentile) of the corresponding cell-type distribution in the training fold (if --label-col provided; labels assigned accordingly).\n",
        "\n",
        "Augment TRAIN with synthetic data. VALIDATION/TEST are NEVER touched.\n",
        "\n",
        "Saves per-fold files under: {output}/folds/fold_{i}/\n",
        "\n",
        "dictionary includes for each fold: train_gen, y_train_gen\n",
        "\n",
        "Notes\n",
        "\n",
        "Model is flow-based with Masked Affine Autoregressive Transform (MAF) as the transform module.\n",
        "\n",
        "All synthesis strictly uses train-only information; no leakage from validation/test.\n",
        "\n",
        "PBMC68K’s processed input must be provided from the ACTIVA repo as used in the paper."
      ],
      "metadata": {
        "id": "KCVvfo4nLwQK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Dpw-sd0IYJA",
        "outputId": "fa23011e-7e3f-4287-efd2-96c9ed7cee59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nflows --quiet"
      ],
      "metadata": {
        "id": "GFy3zax0moUZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "389acb63-4abe-4098-fd89-05eed03d6a5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m124.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for nflows (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrtadgRDIpJk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pylab import savefig\n",
        "from scipy.io import arff\n",
        "import ntpath\n",
        "import glob\n",
        "import os\n",
        "import math\n",
        "from sklearn import preprocessing\n",
        "# !pip install liac-arff\n",
        "#import arff\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from nflows.flows import Flow\n",
        "from nflows.distributions import StandardNormal\n",
        "from nflows.transforms import CompositeTransform, MaskedAffineAutoregressiveTransform\n",
        "\n",
        "from sklearn import manifold\n",
        "import string\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5CV"
      ],
      "metadata": {
        "id": "0ouvMQQERaVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "all_folds = []\n",
        "\n",
        "for fold in range(1, 6):\n",
        "    with open(f\"data/5CV/fold_skf_3000_{fold}.pkl\", \"rb\") as f:\n",
        "        fold_data = pickle.load(f)\n",
        "        all_folds.append(fold_data)\n"
      ],
      "metadata": {
        "id": "eqG203uGReXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,f in enumerate(all_folds, start=1):\n",
        "  print(f\"Fold {i}:\")\n",
        "  print(\"X_train shape:\", f['X_train'].shape)\n",
        "  print(\"y_train shape:\", f['y_train'].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zf8mD2vZRjff",
        "outputId": "0a3b2d3f-e691-497b-bcd4-b86db1618e75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1:\n",
            "X_train shape: (11337, 3000)\n",
            "y_train shape: (11337,)\n",
            "Fold 2:\n",
            "X_train shape: (11337, 3000)\n",
            "y_train shape: (11337,)\n",
            "Fold 3:\n",
            "X_train shape: (11338, 3000)\n",
            "y_train shape: (11338,)\n",
            "Fold 4:\n",
            "X_train shape: (11338, 3000)\n",
            "y_train shape: (11338,)\n",
            "Fold 5:\n",
            "X_train shape: (11338, 3000)\n",
            "y_train shape: (11338,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_values, counts = np.unique(y_train, return_counts=True)\n",
        "display(dict(zip(unique_values, counts)),np.max(counts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "PGW_I_CzMuQL",
        "outputId": "df8b78ad-9e67-4c1a-d6e1-26be7b2843e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{np.str_('PSC'): np.int64(42),\n",
              " np.str_('acinar'): np.int64(1090),\n",
              " np.str_('activated_stellate'): np.int64(227),\n",
              " np.str_('alpha'): np.int64(3897),\n",
              " np.str_('beta'): np.int64(2950),\n",
              " np.str_('delta'): np.int64(759),\n",
              " np.str_('ductal'): np.int64(1360),\n",
              " np.str_('endothelial'): np.int64(231),\n",
              " np.str_('epsilon'): np.int64(17),\n",
              " np.str_('gamma'): np.int64(339),\n",
              " np.str_('macrophage'): np.int64(44),\n",
              " np.str_('mast'): np.int64(20),\n",
              " np.str_('mesenchymal'): np.int64(64),\n",
              " np.str_('pp'): np.int64(148),\n",
              " np.str_('quiescent_stellate'): np.int64(138),\n",
              " np.str_('schwann'): np.int64(11)}"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "np.int64(3897)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PBMC3K"
      ],
      "metadata": {
        "id": "Y-Dr9mcab6MR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(gdrivePath+os.sep+f\"Revision/data/pbmc3k_train.pkl\", \"rb\") as f:\n",
        "    X_train = pickle.load(f)\n",
        "with open(gdrivePath+os.sep+f\"Revision/data/pbmc3k_test.pkl\", \"rb\") as f:\n",
        "    X_test = pickle.load(f)\n",
        "with open(gdrivePath+os.sep+f\"Revision/data/pbmc3k_y_train.pkl\", \"rb\") as f:\n",
        "    y_train = pickle.load(f)\n",
        "with open(gdrivePath+os.sep+f\"Revision/data/pbmc3k_y_test.pkl\", \"rb\") as f:\n",
        "    y_test = pickle.load(f)"
      ],
      "metadata": {
        "id": "HVmBGJiYb53i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PBMC68K"
      ],
      "metadata": {
        "id": "FBLdoNjczOSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install anndata\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKjD60nlh4xa",
        "outputId": "05e1a97b-a641-4332-a258-813dc223d572",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anndata\n",
            "  Downloading anndata-0.12.1-py3-none-any.whl.metadata (9.6 kB)\n",
            "Collecting array-api-compat>=1.7.1 (from anndata)\n",
            "  Downloading array_api_compat-1.12.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: h5py>=3.8 in /usr/local/lib/python3.11/dist-packages (from anndata) (3.14.0)\n",
            "Collecting legacy-api-wrap (from anndata)\n",
            "  Downloading legacy_api_wrap-1.4.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.11/dist-packages (from anndata) (8.4.0)\n",
            "Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.11/dist-packages (from anndata) (2.0.2)\n",
            "Requirement already satisfied: packaging>=24.2 in /usr/local/lib/python3.11/dist-packages (from anndata) (25.0)\n",
            "Requirement already satisfied: pandas!=2.1.2,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from anndata) (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.12 in /usr/local/lib/python3.11/dist-packages (from anndata) (1.16.0)\n",
            "Collecting zarr!=3.0.*,>=2.18.7 (from anndata)\n",
            "  Downloading zarr-3.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.2,>=2.1.0->anndata) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.2,>=2.1.0->anndata) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.2,>=2.1.0->anndata) (2025.2)\n",
            "Collecting donfig>=0.8 (from zarr!=3.0.*,>=2.18.7->anndata)\n",
            "  Downloading donfig-0.8.1.post1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting numcodecs>=0.14 (from numcodecs[crc32c]>=0.14->zarr!=3.0.*,>=2.18.7->anndata)\n",
            "  Downloading numcodecs-0.16.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.9 in /usr/local/lib/python3.11/dist-packages (from zarr!=3.0.*,>=2.18.7->anndata) (4.14.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from donfig>=0.8->zarr!=3.0.*,>=2.18.7->anndata) (6.0.2)\n",
            "Collecting crc32c>=2.7 (from numcodecs[crc32c]>=0.14->zarr!=3.0.*,>=2.18.7->anndata)\n",
            "  Downloading crc32c-2.7.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.2,>=2.1.0->anndata) (1.17.0)\n",
            "Downloading anndata-0.12.1-py3-none-any.whl (169 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.1/169.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading array_api_compat-1.12.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.2/58.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zarr-3.1.0-py3-none-any.whl (254 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading legacy_api_wrap-1.4.1-py3-none-any.whl (10.0 kB)\n",
            "Downloading donfig-0.8.1.post1-py3-none-any.whl (21 kB)\n",
            "Downloading numcodecs-0.16.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading crc32c-2.7.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.7/53.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numcodecs, legacy-api-wrap, donfig, crc32c, array-api-compat, zarr, anndata\n",
            "Successfully installed anndata-0.12.1 array-api-compat-1.12.0 crc32c-2.7.1 donfig-0.8.1.post1 legacy-api-wrap-1.4.1 numcodecs-0.16.1 zarr-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import anndata\n",
        "\n",
        "# Load the h5ad file\n",
        "adata = anndata.read_h5ad(\"/data/68kPBMC_preprocessed.h5ad\")\n",
        "\n",
        "# Print basic info\n",
        "print(adata)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quuR33WSh7on",
        "outputId": "44356acb-d37a-4454-8bbf-feb88530c67f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AnnData object with n_obs × n_vars = 68579 × 17789\n",
            "    obs: 'cluster', 'n_genes', 'n_counts', 'split'\n",
            "    var: 'n_cells'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_mask = adata.obs['split'] == \"train\"\n",
        "test_mask = adata.obs['split'] == \"test\"\n",
        "\n",
        "# Select training data\n",
        "X_train = adata.X[train_mask.values]\n",
        "y_train = adata.obs['cluster'][train_mask.values]\n",
        "\n",
        "# Select test data\n",
        "X_test = adata.X[test_mask.values]\n",
        "y_test = adata.obs['cluster'][test_mask.values]"
      ],
      "metadata": {
        "id": "fzhxgvtNh-3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HCA"
      ],
      "metadata": {
        "id": "obuWhDm3y9Tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "all_folds = []\n",
        "\n",
        "for fold in range(1, 6):\n",
        "    with open(f\"HCA/5CV/fold_skf_{fold}.pkl\", \"rb\") as f:\n",
        "        fold_data = pickle.load(f)\n",
        "        all_folds.append(fold_data)\n"
      ],
      "metadata": {
        "id": "3WLMo6LdpZe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au77ZtT6zJ9H",
        "outputId": "7b7524fa-1e23-474a-e3ff-196035e8001f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8000, 3000), (2000, 3000), (8000,), (2000,))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i,f in enumerate(all_folds, start=1):\n",
        "  print(f\"Fold {i}:\")\n",
        "  print(\"X_train shape:\", f['X_train'].shape)\n",
        "  print(\"y_train shape:\", f['y_train'].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6GEO5yzpenx",
        "outputId": "1c37cb64-da40-4961-b0cf-1b5c26e79395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1:\n",
            "X_train shape: (8000, 3000)\n",
            "y_train shape: (8000,)\n",
            "Fold 2:\n",
            "X_train shape: (8000, 3000)\n",
            "y_train shape: (8000,)\n",
            "Fold 3:\n",
            "X_train shape: (8000, 3000)\n",
            "y_train shape: (8000,)\n",
            "Fold 4:\n",
            "X_train shape: (8000, 3000)\n",
            "y_train shape: (8000,)\n",
            "Fold 5:\n",
            "X_train shape: (8000, 3000)\n",
            "y_train shape: (8000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "def FB_Oversampler(num_synthetic_samples, X_min,num_layers,hidden_features,learning_rate):\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "  X_min = scaler.fit_transform(X_min)\n",
        "\n",
        "  data = torch.tensor(X_min, dtype=torch.float32).to(device)\n",
        "\n",
        "  # Define the number of features and hidden features\n",
        "  num_features = X_min.shape[1]  # Example number of features\n",
        "  #hidden_features = 128  # Example number of hidden units in the neural network\n",
        "\n",
        "  # Create a sequence of masked affine autoregressive transforms\n",
        "  #num_layers = 1\n",
        "\n",
        "  # Base distribution: standard normal\n",
        "  base_distribution = StandardNormal([num_features])\n",
        "\n",
        "  # Define the sequence of transformations\n",
        "  transforms = []\n",
        "  for _ in range(num_layers):\n",
        "      transforms.append(MaskedAffineAutoregressiveTransform(features=num_features, hidden_features=hidden_features))\n",
        "  transform = CompositeTransform(transforms)\n",
        "\n",
        "  # Define the flow model\n",
        "  flow = Flow(transform, base_distribution).to(device)\n",
        "\n",
        "  # Train the model\n",
        "  optimizer = optim.Adam(flow.parameters(), lr=learning_rate)\n",
        "  num_epochs = 100\n",
        "  batch_size = 128 #128\n",
        "\n",
        "\n",
        "  data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "      for batch in data_loader:\n",
        "          batch = batch.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          loss = -flow.log_prob(batch).mean()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "      print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
        "\n",
        "  # Generate new samples\n",
        "  with torch.no_grad():\n",
        "      samples = flow.sample(num_synthetic_samples).to(\"cpu\")\n",
        "  generated_data_np = samples.numpy()\n",
        "\n",
        "   # Rescale the generated samples back to the original scale\n",
        "  generated_data_rescaled = scaler.inverse_transform(generated_data_np)\n",
        "\n",
        "  return generated_data_rescaled"
      ],
      "metadata": {
        "id": "_yqGGdzBq75k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PBMC68K"
      ],
      "metadata": {
        "id": "-9-7yKICI1TO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pbmc68k['X_train']\n",
        "X_test = pbmc68k['X_test']"
      ],
      "metadata": {
        "id": "CVOU62oeTa6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synthetic_samples = FB_Oversampler(int(X_test.shape[0]), X_train,1, 1024, 1e-6)"
      ],
      "metadata": {
        "id": "2WM75o_DcI6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synthetic_samples.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqHtEX3Dcasf",
        "outputId": "52685723-a2c7-4736-e432-854602a208a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6991, 3000)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(f\"results/pbmc68k_MAF-FB_generated.pkl\", \"wb\") as f:\n",
        "   pickle.dump(synthetic_samples,f)"
      ],
      "metadata": {
        "id": "mSXdUd5KcZfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5CV RUN"
      ],
      "metadata": {
        "id": "CYWHKPBHRzNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def FB_CV():\n",
        "  gen_dict = []\n",
        "  for k, fold in enumerate(all_folds, start=1):\n",
        "      X_train = fold['X_train']\n",
        "      X_val = fold['X_val']\n",
        "      y_train = fold['y_train']\n",
        "      y_val = fold['y_val']\n",
        "\n",
        "\n",
        "      # Get unique values and their counts\n",
        "      unique_values, counts = np.unique(y_train, return_counts=True)\n",
        "      classlabel_counts = dict(zip(unique_values, counts))\n",
        "      Q1, Q2, Q3 = np.quantile(counts, [0.25, 0.5, 0.75], axis=0, method='nearest')\n",
        "      max_count = Q3 #np.max(counts)\n",
        "\n",
        "      i=1\n",
        "      for label, count in classlabel_counts.items():\n",
        "          #print(x.shape, y.shape)\n",
        "          print(\"label, count, max_count\",label,count,max_count)\n",
        "          X_minority = X_train[y_train == label]\n",
        "          if count < max_count:\n",
        "              #print(\"\\n\")\n",
        "              #print(f\"Value {label} appears {count} times.\")\n",
        "              num_synthetic_samples = max_count - count\n",
        "              synthetic_samples = FB_Oversampler(int(num_synthetic_samples), X_minority,1, 1024, 1e-6)\n",
        "\n",
        "              X_minority = np.array(X_minority)\n",
        "              synthetic_samples = np.array(synthetic_samples)\n",
        "              if i==1:\n",
        "                X_train_gen = X_minority\n",
        "                X_train_gen = np.vstack([X_train_gen, synthetic_samples])\n",
        "\n",
        "                y_train_gen = np.full(max_count, label)\n",
        "\n",
        "                y_train_indexes = np.full(count, 1)\n",
        "                y_train_indexes = np.concatenate([y_train_indexes, np.full(num_synthetic_samples, 2)])\n",
        "              else:\n",
        "                tmp = np.vstack([X_minority, synthetic_samples])\n",
        "                X_train_gen = np.vstack([X_train_gen, tmp])\n",
        "                y_train_gen = np.concatenate([y_train_gen, np.full(max_count, label)])\n",
        "\n",
        "                y_train_indexes = np.concatenate([y_train_indexes, np.full(count, 1)])\n",
        "                y_train_indexes = np.concatenate([y_train_indexes, np.full(num_synthetic_samples, 2)])\n",
        "          else:\n",
        "              X_train_gen = np.vstack([X_train_gen, X_minority])\n",
        "              y_train_gen = np.concatenate([y_train_gen, np.full(count, label)])\n",
        "              y_train_indexes = np.concatenate([y_train_indexes, np.full(count, 1)])\n",
        "          i=i+1\n",
        "\n",
        "      syn = {\n",
        "          'X_train_gen': X_train_gen,\n",
        "          'y_train_gen': y_train_gen\n",
        "      }\n",
        "      gen_dict.append(syn)\n",
        "\n",
        "      with open(\"HCA/5CV\" + os.sep + f'MAF-FB_skf_fold_'+str(k)+'.pkl', 'wb') as f:\n",
        "          pickle.dump(syn, f)\n",
        "  return gen_dict\n"
      ],
      "metadata": {
        "id": "OiI5s8brR0Xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_dict = FB_CV()"
      ],
      "metadata": {
        "id": "fwhGt6PSSI9l"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}