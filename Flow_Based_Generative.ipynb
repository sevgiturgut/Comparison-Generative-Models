{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Dpw-sd0IYJA","outputId":"e062f3fb-d0a8-4abd-e225-730a3ce86662"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GFy3zax0moUZ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a2365421-e8bd-424f-d852-3abddbc71358"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for nflows (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install nflows --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FrtadgRDIpJk"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from pylab import savefig\n","from scipy.io import arff\n","import ntpath\n","import glob\n","import os\n","import math\n","from sklearn import preprocessing\n","# !pip install liac-arff\n","#import arff\n","import argparse\n","\n","import torch\n","from torch import nn, optim\n","from nflows.flows import Flow\n","from nflows.distributions import StandardNormal\n","from nflows.transforms import CompositeTransform, MaskedAffineAutoregressiveTransform, RandomPermutation\n","from nflows.transforms import AffineCouplingTransform\n","\n","from sklearn import manifold\n","import string\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dPH75F5p9O31"},"outputs":[],"source":["import pickle\n","# Load object from file\n","with open(gdrivePath +os.sep + \"data\"+os.sep+'X_train.pkl', 'rb') as f:\n","    X_train = pickle.load(f)\n","with open(gdrivePath +os.sep + \"data\" +os.sep+'X_test.pkl', 'rb') as f:\n","    X_test = pickle.load(f)\n","with open(gdrivePath +os.sep + \"data\"+os.sep+'y_train.pkl', 'rb') as f:\n","    y_train = pickle.load(f)\n","with open(gdrivePath +os.sep + \"data\"+os.sep+'y_test.pkl', 'rb') as f:\n","    y_test = pickle.load(f)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mVuN8FDW1sKJ"},"outputs":[],"source":["X_train.shape, y_train.shape, X_test.shape, y_test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PGW_I_CzMuQL"},"outputs":[],"source":["unique_values, counts = np.unique(y_train, return_counts=True)\n","display(dict(zip(unique_values, counts)),np.max(counts))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z2z8PBvosi-K"},"outputs":[],"source":["X_train[0].shape"]},{"cell_type":"code","source":["y_train"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LNp9OvMoX_Cy","outputId":"4cfd267f-e37e-4e17-a12b-1d17d164b65e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['ductal', 'acinar', 'delta', ..., 'beta', 'beta', 'beta'],\n","      dtype=object)"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["from sklearn.preprocessing import OneHotEncoder\n","\n","\n","# Reshape the array to 2D (required for OneHotEncoder)\n","y_reshaped = y_train.reshape(-1, 1)\n","\n","# Initialize the OneHotEncoder\n","encoder = OneHotEncoder(sparse_output=False)\n","\n","# Fit and transform the data\n","y_one_hot_ori = encoder.fit_transform(y_reshaped)\n","\n","# Print the one-hot encoded array\n","print(\"One-Hot Encoded Array:\")\n","print(y_one_hot_ori)\n","\n","# Optionally, print the categories\n","print(\"Categories:\")\n","print(encoder.categories_)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X_eypNmLYLZ7","outputId":"919d444e-83c9-4fe6-a90c-544152995d79"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["One-Hot Encoded Array:\n","[[0. 0. 0. ... 0. 0. 0.]\n"," [0. 1. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n","Categories:\n","[array(['PSC', 'acinar', 'activated_stellate', 'alpha', 'beta', 'delta',\n","       'ductal', 'endothelial', 'epsilon', 'gamma', 'macrophage', 'mast',\n","       'mesenchymal', 'pp', 'quiescent_stellate', 'schwann'], dtype=object)]\n"]}]},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","\n","# Step 1: Encode the categories as integers\n","label_encoder = LabelEncoder()\n","y_encoded = label_encoder.fit_transform(y_train)  # Convert categories to integers\n","\n","# Step 2: Convert the encoded labels to a torch.LongTensor\n","y_tensor = torch.tensor(y_encoded, dtype=torch.long)"],"metadata":{"id":"WMCaoloQoASZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_tensor"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ByDgZhDfo32k","outputId":"47fb0ce0-fc9d-482b-c7d7-7d789d26abf0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([6, 1, 5,  ..., 4, 4, 4])"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GYSfua9svS1g","outputId":"f610a3fb-97a1-4346-eb59-7c5c66e67faf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["import torch\n","import torch.optim as optim\n","import numpy as np\n","from nflows.distributions import StandardNormal\n","from nflows.transforms import MaskedAffineAutoregressiveTransform, ReversePermutation, CompositeTransform\n","from nflows.flows import Flow\n","\n","\n","# Assuming X_moons and label_moon are already defined\n","# `label_moon` contains class labels (e.g., 0, 1, 2, ..., n_classes - 1)\n","n_features = X_train.shape[1]\n","n_layers = 1\n","n_classes = len(np.unique(y_tensor))  # Determine the number of classes\n","\n","# Base distribution\n","base_dist = StandardNormal(shape=[n_features])\n","\n","# Define the transform\n","transforms = []\n","for i in range(n_layers):\n","    transforms.append(\n","        MaskedAffineAutoregressiveTransform(\n","            features=n_features,\n","            hidden_features=1024,\n","            context_features=n_classes  # Updated for multi-class\n","        )\n","    )\n","    transforms.append(ReversePermutation(features=n_features))\n","\n","transform = CompositeTransform(transforms)\n","\n","# Define the flow\n","flow = Flow(transform, base_dist).to(device)\n","\n","# Optimizer\n","optimizer = optim.Adam(flow.parameters(),lr=1e-6)\n","\n","# Training parameters\n","num_epochs = 50\n","batch_size = 128\n","max_batches = int(X_train.shape[0] / batch_size)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    permut = np.random.permutation(X_train.shape[0])\n","    X_moons_shuffle = X_train[permut]\n","    label_moon_shuffle = y_tensor[permut]\n","\n","    for i_batch in range(max_batches):\n","        # Get the batch\n","        x = X_moons_shuffle[i_batch * batch_size:(i_batch + 1) * batch_size]\n","        x = torch.tensor(x,device=device).float()\n","\n","        y = label_moon_shuffle[i_batch * batch_size:(i_batch + 1) * batch_size]\n","        y_one_hot = torch.nn.functional.one_hot(\n","            torch.tensor(y,device=device), num_classes=n_classes\n","        ).float()  # One-hot encoding\n","\n","        # Zero gradients\n","        optimizer.zero_grad()\n","\n","        # Compute negative log likelihood\n","        nll = -flow.log_prob(x, context=y_one_hot)  # Use one-hot encoded context\n","        loss = nll.mean()\n","\n","        # Backpropagation\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Logging\n","    if epoch % 10 == 0:\n","        print(f\"Epoch: {epoch}, Loss: {loss.item():.4f}\")\n"],"metadata":{"id":"apueta8lZddT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["unique_values, counts = np.unique(y_train, return_counts=True)\n","#display(dict(zip(unique_values, counts)),np.max(counts))\n","counts"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y00MfZ2MxXGW","outputId":"d7d3ffef-9dd0-4c24-892a-d2e0229e583f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  42, 1090,  227, 3897, 2950,  759, 1360,  231,   17,  339,   44,\n","         20,   64,  148,  138,   11])"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["threshold = 759\n","\n","# Calculate samples_per_class\n","#samples_per_class = np.maximum(threshold - counts, 0)\n","\n","# Threshold value\n","threshold = 759\n","\n","# Create a dictionary for samples_per_class\n","samples_per_class = {}\n","\n","# Loop through the counts and calculate the number of samples to generate\n","for class_id, count in enumerate(counts):\n","    # If count is less than the threshold, calculate how many samples to generate\n","    if count < threshold:\n","        samples_per_class[class_id] = threshold - count\n","    else:\n","        samples_per_class[class_id] = 0  # No samples needed for this class\n","\n","# Print the result\n","print(\"Samples Per Class:\", samples_per_class)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aY_AN8MKy9tu","outputId":"1fb2aea8-65c8-4f5a-b6b8-d1e70bf6b6b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Samples Per Class: {0: 717, 1: 0, 2: 532, 3: 0, 4: 0, 5: 0, 6: 0, 7: 528, 8: 742, 9: 420, 10: 715, 11: 739, 12: 695, 13: 611, 14: 621, 15: 748}\n"]}]},{"cell_type":"code","source":["# Create the context tensor\n","cond_list = []\n","for class_id, num_samples in samples_per_class.items():\n","    # Create one-hot encoded context for the class\n","    class_context = torch.nn.functional.one_hot(\n","        torch.tensor([class_id] * num_samples,dtype=torch.long), num_classes=n_classes\n","    )\n","    cond_list.append(class_context)\n","\n","# Concatenate all contexts\n","cond = torch.cat(cond_list, dim=0).float().to(device)\n","\n","# Generate samples using the flow model\n","with torch.no_grad():\n","    samples = flow.sample(1, context=cond).view(-1, n_features).cpu().numpy()\n","\n","# Print the shape of the generated samples\n","#print(samples.shape)  # Should match the total number of samples (e.g., 100 + 50 + 150 = 300)\n"],"metadata":{"id":"4SDiYT9EqStp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["combined = np.vstack((X_train, samples))"],"metadata":{"id":"5t4oRjpw3vHC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_one_hot_ori.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FlkEfR3C4L4t","outputId":"a32d451f-a785-4dd1-94a9-bf3c19c131af"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(11337, 16)"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["cond_npy = cond.cpu().numpy()\n","cond_npy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ja6bvLV-4hwk","outputId":"1e39c6cd-7f3d-466a-9087-d757a78b1f83"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1., 0., 0., ..., 0., 0., 0.],\n","       [1., 0., 0., ..., 0., 0., 0.],\n","       [1., 0., 0., ..., 0., 0., 0.],\n","       ...,\n","       [0., 0., 0., ..., 0., 0., 1.],\n","       [0., 0., 0., ..., 0., 0., 1.],\n","       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["cond_npy.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w06zC5Du4-S5","outputId":"9d261c6d-50b6-4db3-dd05-1d3a39374cb0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(7068, 16)"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["combined_y = np.vstack((y_one_hot_ori, cond_npy))\n","combined_y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r_PF-8o75AdE","outputId":"401428ba-10ff-4f70-f7fc-a07dd5b83db2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 0., 0., ..., 0., 0., 0.],\n","       [0., 1., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       ...,\n","       [0., 0., 0., ..., 0., 0., 1.],\n","       [0., 0., 0., ..., 0., 0., 1.],\n","       [0., 0., 0., ..., 0., 0., 1.]])"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["print(combined.shape, combined_y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"29Rg_aFk5JhG","outputId":"4805f279-fcdb-48b6-e9db-b302de828f58"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(18405, 7514) (18405, 16)\n"]}]},{"cell_type":"code","source":["y_train_indexes = np.full(len(y_one_hot_ori), 1)\n","y_train_indexes = np.concatenate([y_train_indexes, np.full(len(samples), 2)])"],"metadata":{"id":"Bqt0SgRs5gd5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train_indexes.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gaE6AtYX5oMa","outputId":"25802a0f-6d8b-43cb-8d2c-fb447b85c536"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(18405,)"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["file_name =\"FB\""],"metadata":{"id":"fGzUjkjfUi1T"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2WKo4iP-9eEZ"},"outputs":[],"source":["import pickle\n","\n","# Save object to file\n","with open(gdrivePath +os.sep+\"results\"+os.sep+\"FB\"+os.sep+file_name+'_X_train.pkl', 'wb') as f:\n","    pickle.dump(combined, f)\n","\n","with open(gdrivePath +os.sep+\"results\"+os.sep+\"FB\"+os.sep+file_name+'_y_train.pkl', 'wb') as f:\n","    pickle.dump(combined_y, f)\n","\n","with open(gdrivePath +os.sep+\"results\"+os.sep+\"FB\"+os.sep+file_name+'_y_train_idx.pkl', 'wb') as f:\n","    pickle.dump(y_train_indexes, f)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}